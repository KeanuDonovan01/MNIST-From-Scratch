{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data into dataframe\n",
    "train_data = pd.read_csv(r'data\\train.csv')\n",
    "test_data = pd.read_csv(r'data\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into labels and inputs\n",
    "labels = train_data['label'].values\n",
    "inputs = train_data.drop('label', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_index = 1\n",
    "image = inputs[image_index].reshape(28, 28)\n",
    "\n",
    "# Create a smaller figure and axis\n",
    "fig, ax = plt.subplots(figsize=(3, 3))  # Reduced from (6, 6) to (3, 3)\n",
    "\n",
    "# Display the image\n",
    "im = ax.imshow(image, cmap='gray')\n",
    "\n",
    "# Remove axis ticks for a cleaner look\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Add a title\n",
    "ax.set_title(f\"MNIST Digit (Index: {image_index})\")\n",
    "\n",
    "# Adjust the layout to prevent cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each label\n",
    "label_counts = Counter(labels)\n",
    "# Sort the counts by label\n",
    "sorted_counts = sorted(label_counts.items())\n",
    "# Separate the labels and counts\n",
    "x, y = zip(*sorted_counts)\n",
    "# Create the bar plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(x, y)\n",
    "\n",
    "#Customize the plot\n",
    "ax.set_xlabel('Digit')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Labels in MNIST Dataset')\n",
    "ax.set_xticks(range(10))\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, v in enumerate(y):\n",
    "    ax.text(i, v, str(v), ha='center', va='bottom')\n",
    "\n",
    "# Add a grid for better readability\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels):\n",
    "    #Get unique labels and create a mapping from label to index\n",
    "    unique_labels, label_indicies = np.unique(labels, return_inverse=True)\n",
    "\n",
    "    #Use np.eye to create one-hot encoded matrix\n",
    "    one_hot = np.eye(len(unique_labels))[label_indicies]\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    num_samples = X.shape[0]\n",
    "    num_test = int(num_samples * test_size)\n",
    "    \n",
    "    # Create random permutation of indices\n",
    "    indices = np.random.permutation(num_samples)\n",
    "    \n",
    "    # Split indices\n",
    "    test_indices = indices[:num_test]\n",
    "    train_indices = indices[num_test:]\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encode\n",
    "one_hot_encode(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLu:\n",
    "    def forward(self, inputs):\n",
    "        # Store the input values for later use in backpropagation\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # Apply the ReLU activation function\n",
    "        # ReLU returns 0 for any negative input, and the input itself for any positive input\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "        return self.output  # Make sure to return the output\n",
    "        \n",
    "        # The np.maximum function compares each element of the inputs array with 0\n",
    "        # and returns an array with the maximum of the two for each element\n",
    "        \n",
    "        # This effectively \"activates\" neurons: \n",
    "        # - Neurons with negative inputs are not activated (output 0)\n",
    "        # - Neurons with positive inputs are activated (output equals input)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify the original variable,\n",
    "        # let's make a copy of the values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "        return self.dinputs\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_CategoricalCrossEntropy():\n",
    "    def forward(self, inputs, y_true):\n",
    "        #get unnormalised probabilites, ensures there is no overflow\n",
    "        #Softmax part\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        #normalised probabilites\n",
    "        probabilites = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilites\n",
    "\n",
    "        #CategoricalCrossEntropy part\n",
    "\n",
    "        #clip data to prevent division by 0\n",
    "        #Clip both sides to prevent mean being dragged to any value\n",
    "        y_pred_clipped = np.clip(probabilites, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        #Probabilites for target values\n",
    "        #only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(len(probabilites)), y_true]\n",
    "        # Mask values - only for one-hot encoded labels, sums across the row\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        #Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        #number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        #If labels are one-hot encoded\n",
    "        #Finds the max value along each row and returns a 1D array where each element is the index of the 1 in the corresponding row\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        \n",
    "        #copy to modify safely\n",
    "        self.dinputs = dvalues.copy()\n",
    "        #calculate gradient\n",
    "        #calulate gradient\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "        return self.dinputs\n",
    "\n",
    "    def calculate(self, inputs, y):\n",
    "        sample_losses = self.forward(inputs, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer():\n",
    "    def __init__(self, n_inputs, n_neurons, l1=0, l2=0):\n",
    "        # He initialization for weights\n",
    "        # The formula is: weight = random_normal() * sqrt(2 / n_inputs)\n",
    "        # This helps maintain the variance of activations through the network\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons) * np.sqrt(2.0 / n_inputs)\n",
    "\n",
    "        #biases shape (1, n_neurons)\n",
    "        #intialise to zero\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        self.dweights = None\n",
    "        self.dbiases = None\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        #store input values for use in backward pass\n",
    "        self.inputs = inputs\n",
    "        #Compute output  y = x*W +b\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # dvalues shape: (batch_size, n_neurons)\n",
    "\n",
    "        # Gradients on parameters\n",
    "        # self.inputs shape: (batch_size, n_inputs)\n",
    "        # dvalues shape: (batch_size, n_neurons)\n",
    "        # self.dweights shape: (n_inputs, n_neurons)\n",
    "\n",
    "        #Need to transpose self.inputs to match the shape of dvalues\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "\n",
    "        #Gradients on biases\n",
    "        #Sum vertically (for each neuron)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        # L1 regularization on weights\n",
    "        if self.l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.l1 * dL1\n",
    "\n",
    "        # L2 regularization on weights\n",
    "        if self.l2 > 0:\n",
    "            self.dweights += 2 * self.l2 * self.weights\n",
    "\n",
    "        # Gradient on inputs\n",
    "        # self.weights shape: (n_inputs, n_neurons)\n",
    "        # dvalues shape: (batch_size, n_neurons)\n",
    "        # self.dinputs shape: (batch_size, n_inputs)\n",
    "\n",
    "        #Need to transpose \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "        return self.dinputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam():\n",
    "    def __init__(self, learning_rate=0.001, decay=0, epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        # Correct the error here: Change multiplication to division\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1 - rate  # Store the keep rate (1 - dropout rate)\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Create a mask of 1's and 0's with probability of keeping a node = self.rate\n",
    "        self.mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        return inputs * self.mask  # Apply mask to inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Apply the same mask to the gradient\n",
    "        return dvalues * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, l1=0, l2=0, learning_rate=0.001, decay=1e-5):\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        \n",
    "        # Define the network architecture\n",
    "        self.layers = [\n",
    "            DenseLayer(784, 128, l1=self.l1, l2=self.l2),\n",
    "            Activation_ReLu(),\n",
    "            Dropout(0.1),  #dropout rate\n",
    "            DenseLayer(128, 64, l1=self.l1, l2=self.l2),\n",
    "            Activation_ReLu(),\n",
    "            Dropout(0.1),  # dropout rate\n",
    "            DenseLayer(64, 32, l1=self.l1, l2=self.l2),\n",
    "            Activation_ReLu(),\n",
    "            DenseLayer(32, 10, l1=self.l1, l2=self.l2),\n",
    "        ]\n",
    "        \n",
    "        # Define loss function\n",
    "        self.loss_activation = Softmax_CategoricalCrossEntropy()\n",
    "        \n",
    "        # Initialize optimizer with provided learning rate and decay\n",
    "        self.optimizer = Optimizer_Adam(learning_rate=self.learning_rate, decay=self.decay)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        # Forward pass\n",
    "        output = self.forward(X, training=False)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = self.loss_activation.calculate(output, y)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predictions = np.argmax(output, axis=1)\n",
    "        accuracy = np.mean(predictions == np.argmax(y, axis=1))\n",
    "        \n",
    "        return loss, accuracy\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        # Forward pass through all layers\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                # Apply dropout only during training\n",
    "                X = layer.forward(X) if training else X\n",
    "            else:\n",
    "                X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, output, y):\n",
    "        # Backward pass starting from the loss\n",
    "        dinputs = self.loss_activation.backward(output, y)\n",
    "        for layer in reversed(self.layers):\n",
    "            dinputs = layer.backward(dinputs)\n",
    "        return dinputs\n",
    "    \n",
    "    def train(self, X_train, y_train, X_test, y_test, epochs, batch_size):\n",
    "        total_start_time = time.time()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            # Shuffle the training data\n",
    "            permutation = np.random.permutation(len(X_train))\n",
    "            X_shuffled = X_train[permutation]\n",
    "            y_shuffled = y_train[permutation]\n",
    "\n",
    "            # Train in batches\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                output = self.forward(X_batch, training=True)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.backward(output, y_batch)\n",
    "\n",
    "                # Update parameters\n",
    "                self.optimizer.pre_update_params()\n",
    "                for layer in self.layers:\n",
    "                    if isinstance(layer, DenseLayer):\n",
    "                        self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update_params()\n",
    "\n",
    "            # Evaluate the model after each epoch\n",
    "            train_loss, train_accuracy = self.evaluate(X_train, y_train)\n",
    "            test_loss, test_accuracy = self.evaluate(X_test, y_test)\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            epoch_duration = epoch_end_time - epoch_start_time\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, \"\n",
    "                f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "                f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, \"\n",
    "                f\"Time: {epoch_duration:.2f} seconds\")\n",
    "        \n",
    "        total_end_time = time.time()\n",
    "        total_duration = total_end_time - total_start_time\n",
    "        print(f\"\\nTotal training time: {total_duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X):\n",
    "    # Forward pass\n",
    "    output = model.forward(X, training=False)\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = np.argmax(output, axis=1)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(predictions, filename='submission.csv'):\n",
    "    # Create a DataFrame with the predictions\n",
    "    submission = pd.DataFrame({\n",
    "        'ImageId': range(1, len(predictions) + 1),\n",
    "        'Label': predictions\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f\"Submission saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(inputs, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize inputs\n",
    "X_train = X_train_raw / 255.0\n",
    "X_test = X_test_raw / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = one_hot_encode(y_train_raw)\n",
    "y_test = one_hot_encode(y_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters \n",
    "l1 = 1e-8\n",
    "l2 = 0.000001\n",
    "learning_rate = 3e-4\n",
    "decay_rate = 3e-6\n",
    "epochs = 30\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(l1, l2, learning_rate, decay_rate )\n",
    "\n",
    "print(\"Network structure:\")\n",
    "for i, layer in enumerate(nn.layers):\n",
    "    print(f\"Layer {i}: {type(layer).__name__}\")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "nn = NeuralNetwork(l1, l2, learning_rate, decay_rate)\n",
    "nn.train(X_train, y_train, X_test, y_test, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating test.csv\n",
    "# Load test data\n",
    "test_data = pd.read_csv(r'data\\test.csv')\n",
    "\n",
    "# Preprocess test data\n",
    "X_test = test_data.values.astype('float32')\n",
    "X_test /= 255.0  # Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "test_predictions = predict(nn, X_test)\n",
    "\n",
    "print(\"Test predictions shape:\", test_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
